{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results across hyperparameters\n",
    "Collect performance information like F1 scores for a given model architecture while varying the hyperparameters in the objective function that weight the weak and self-supervised terms; plot performance several different ways in a heatmap format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "from daart.data import DataGenerator\n",
    "from daart.eval import get_precision_recall, run_lengths\n",
    "from daart.io import get_expt_dir\n",
    "from daart.transforms import ZScore\n",
    "from daart.utils import compute_batch_pad\n",
    "\n",
    "from daart_utils.data import DataHandler\n",
    "from daart_utils.models import compute_model_predictions\n",
    "from daart_utils.paths import data_path, results_path\n",
    "from daart_utils.plotting import plot_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save predicted states from models\n",
    "save_states = True\n",
    "# overwrite predicted states from models\n",
    "overwrite_states = False\n",
    "# compute state statistics like median bout duration and behavior ratios\n",
    "compute_state_stats = False\n",
    "\n",
    "dataset = 'fly'\n",
    "input_type = 'markers'  # 'markers' | 'features-simba' | etc.\n",
    "\n",
    "if dataset == 'fly':\n",
    "    from daart_utils.session_ids.fly import SESS_IDS_TRAIN_5, SESS_IDS_TEST\n",
    "    from daart_utils.session_ids.fly import label_names\n",
    "elif dataset == 'ibl':\n",
    "    from daart_utils.session_ids.ibl import SESS_IDS_TRAIN_5, SESS_IDS_TEST\n",
    "    from daart_utils.session_ids.ibl import label_names\n",
    "    \n",
    "# train datasets\n",
    "sess_ids = SESS_IDS_TRAIN_5[0]\n",
    "# test datasets\n",
    "sess_ids_test = SESS_IDS_TEST\n",
    "\n",
    "expt_dir = get_expt_dir(os.path.join(results_path, dataset), sess_ids)\n",
    "print(expt_dir)\n",
    "\n",
    "# ATTN: need to set these params to match models that were fit\n",
    "lambda_weak = [0, 1]\n",
    "lambda_strong = 1\n",
    "lambda_pred = [0, 1]\n",
    "\n",
    "model_type = 'dtcn'\n",
    "device = 'cuda'\n",
    "batch_size = 2000\n",
    "tt_expt_dir = 'test'\n",
    "\n",
    "hparams = {\n",
    "    'input_type': input_type,\n",
    "    'rng_seed_train': 0,\n",
    "    'rng_seed_model': 0,\n",
    "    'trial_splits': '9;1;0;0',\n",
    "    'train_frac': 1,\n",
    "    'batch_size': batch_size,\n",
    "    'model_type': model_type,\n",
    "    'learning_rate': 1e-4,\n",
    "    'n_hid_layers': 2,\n",
    "    'n_hid_units': 32,\n",
    "    'n_lags': 4,\n",
    "    'l2_reg': 0,\n",
    "    'lambda_strong': lambda_strong,\n",
    "    'bidirectional': True,\n",
    "    'device': device,\n",
    "    'dropout': 0.1,\n",
    "    'activation': 'lrelu'\n",
    "}\n",
    "\n",
    "hparams['batch_pad'] = compute_batch_pad(hparams)\n",
    "hparams['tt_expt_dir'] = os.path.join(expt_dir, model_type, tt_expt_dir)\n",
    "model_types = [model_type]  # legacy, need to clean up\n",
    "\n",
    "metrics_df = []\n",
    "\n",
    "for sess_id_test in sess_ids_test:\n",
    "    \n",
    "    # initialize data handler; point to correct base path\n",
    "    handler = DataHandler(sess_id_test, base_path=os.path.join(data_path, dataset))\n",
    "    if input_type == 'markers':\n",
    "        markers_file = handler.get_marker_filepath()\n",
    "    else:\n",
    "        markers_file = handler.get_feature_filepath(dirname=input_type)\n",
    "\n",
    "    # define data generator signals\n",
    "    signals = ['markers']\n",
    "    transforms = [ZScore()]\n",
    "    paths = [markers_file]\n",
    "\n",
    "    # build data generator\n",
    "    data_gen_test = DataGenerator(\n",
    "        [sess_id_test], [signals], [transforms], [paths], device=device, \n",
    "        batch_size=batch_size, trial_splits=hparams['trial_splits'], \n",
    "        batch_pad=hparams['batch_pad'], input_type=hparams['input_type'])\n",
    "    print('----------------------------')\n",
    "    print(data_gen_test)\n",
    "    print('----------------------------')\n",
    "    print('\\n')\n",
    "\n",
    "    # load hand labels\n",
    "    handler.load_hand_labels()\n",
    "    states = np.argmax(handler.hand_labels.vals, axis=1)\n",
    "    cutoff = int(np.floor(states.shape[0] / batch_size)) * batch_size\n",
    "    states = states[:cutoff]\n",
    "\n",
    "    # load heuristic labels\n",
    "    handler.load_heuristic_labels()\n",
    "    states_h = np.argmax(handler.heuristic_labels.vals, axis=1)\n",
    "    states_heuristic = states_h[:cutoff]\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # collect results\n",
    "    # -----------------------------------------        \n",
    "    for lw in lambda_weak:\n",
    "        for lp in lambda_pred:\n",
    "\n",
    "            hparams['lambda_weak'] = lw\n",
    "            hparams['lambda_pred'] = lp\n",
    "            \n",
    "            # f1, precision, accuracy\n",
    "            scores = {m: None for m in model_types}\n",
    "            # ratio of time spent in each state\n",
    "            ratios = {m: None for m in model_types}\n",
    "            # median bout length for each state\n",
    "            bout_lens = {m: None for m in model_types}\n",
    "\n",
    "            # load/compute predictions\n",
    "            try:\n",
    "                predictions = compute_model_predictions(\n",
    "                    hparams, data_gen_test, save_states=save_states, \n",
    "                    overwrite_states=overwrite_states)\n",
    "            except (FileNotFoundError, NotADirectoryError):\n",
    "                continue\n",
    "\n",
    "            # compute precision and recall for each behavior type (model)\n",
    "            scores[model_type] = get_precision_recall(\n",
    "                states, predictions, background=0, n_classes=len(label_names) - 1)\n",
    "\n",
    "            if compute_state_stats:\n",
    "                ratios[model_type] = np.bincount(predictions) / len(predictions)\n",
    "                bouts = run_lengths(predictions)\n",
    "                bout_lens[model_type] = [np.median(a) for _, a in bouts.items()]\n",
    "            else:\n",
    "                ratios[model_type] = [None] * (len(label_names) + 1)\n",
    "                bout_lens[model_type] = [None] * (len(label_names) + 1)\n",
    "\n",
    "            # compute precision and recall for each behavior type (heuristic)\n",
    "            scores['heuristic'] = get_precision_recall(\n",
    "                states, states_heuristic, background=0, n_classes=len(label_names) - 1)\n",
    "\n",
    "            # store\n",
    "            for l, label_name in enumerate(label_names[1:]):\n",
    "                df_dict = {\n",
    "                    'sess_id': sess_id_test,\n",
    "                    'label': label_name,\n",
    "                    'lambda_weak': lw,\n",
    "                    'lambda_pred': lp,\n",
    "                    'precision_heur': scores['heuristic']['precision'][l],\n",
    "                    'recall_heur': scores['heuristic']['recall'][l],\n",
    "                    'f1_heur': scores['heuristic']['f1'][l],\n",
    "                }\n",
    "                for model_type in model_types:\n",
    "                    model_name = 'mlp' if model_type == 'temporal-mlp' else model_type\n",
    "                    df_dict['precision_%s' % model_name] = scores[model_type]['precision'][l]\n",
    "                    df_dict['recall_%s' % model_name] = scores[model_type]['recall'][l]\n",
    "                    df_dict['f1_%s' % model_name] = scores[model_type]['f1'][l]\n",
    "                    df_dict['ratio_%s' % model_name] = ratios[model_type][l + 1]\n",
    "                    df_dict['bout_len_%s' % model_name] = bout_lens[model_type][l + 1]\n",
    "                metrics_df.append(pd.DataFrame(df_dict, index=[0]))\n",
    "\n",
    "metrics_df = pd.concat(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot results: heat plots for single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "metric = 'f1_dtcn'\n",
    "\n",
    "n_rows = 2\n",
    "n_cols = 2\n",
    "\n",
    "# 1: by dataset/label\n",
    "# 2: by label, avg over datasets\n",
    "# 3: avg over dataset/label\n",
    "plot_types = [2, 3]\n",
    "\n",
    "for plot_type in plot_types:\n",
    "    annot = True #True if plot_type == 3 else False\n",
    "    plot_heatmaps(\n",
    "        df=metrics_df, metric=metric, sess_ids=sess_ids_test, \n",
    "        title=metric.split('_')[-1].upper(), kind=plot_type, vmin=0.7, vmax=1,\n",
    "        annot=annot, \n",
    "        cmaps=['Oranges_r', 'Greens_r', 'Reds_r', 'Purples_r', 'Blues_r', 'Greys_r'],\n",
    "        save_file=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daart",
   "language": "python",
   "name": "daart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
