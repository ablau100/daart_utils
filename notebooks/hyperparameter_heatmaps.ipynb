{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from daart.data import DataGenerator\n",
    "from daart.eval import get_precision_recall, run_lengths\n",
    "from daart.io import get_expt_dir, find_experiment\n",
    "from daart.models import Segmenter\n",
    "from daart.transforms import ZScore\n",
    "from daart_scratch.plotting import plot_heatmaps\n",
    "\n",
    "from daart_scratch.session_ids_ibl import EXPT_IDS_TRAIN_5, EXPT_IDS_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results from different model types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save predicted states from models\n",
    "save_states = True\n",
    "# overwrite predicted states from models\n",
    "overwrite_states = False\n",
    "# compute state statistics like median bout duration and behavior ratios\n",
    "compute_state_stats = False\n",
    "\n",
    "base_dir = '/media/mattw/ibl/segmentation'\n",
    "# tt_expt_dir = 'grid-search'\n",
    "tt_expt_dir = 'batch=2000_lr=1e-4'\n",
    "\n",
    "# define training datasets\n",
    "# expt_ids = [\n",
    "#     '2019_06_26_fly2',\n",
    "# #     '2019_08_14_fly1',\n",
    "# #     '2019_10_21_fly1'\n",
    "# #     '2019_08_08_fly1',\n",
    "# ]\n",
    "# expt_ids = EXPT_IDS_TRAIN_10[0]\n",
    "expt_ids = EXPT_IDS_TRAIN_5[0]\n",
    "# expt_ids = EXPT_IDS_TRAIN_3[0]\n",
    "\n",
    "# test datasets\n",
    "# expt_ids_test = EXPT_IDS_TRAIN_5[0]\n",
    "expt_ids_test = EXPT_IDS_TEST\n",
    "# expt_ids_test = [\n",
    "#     'cortexlab_KS020_2020-02-06-001',\n",
    "#     'danlab_DY_009_2020-02-27-001',\n",
    "#     'mrsicflogellab_SWC_058_2020-12-11-001',\n",
    "# ]\n",
    "\n",
    "expt_dir = get_expt_dir(os.path.join(base_dir, 'classifiers'), expt_ids)\n",
    "print(expt_dir)\n",
    "\n",
    "model_types = ['dtcn'] # , 'dtcn'] #, 'dtcn'] # 'temporal-mlp', 'gru'] #, 'lstm', 'gru']\n",
    "lambda_weak = [0, 0.1, 0.5, 1, 5]\n",
    "lambda_strong = 1\n",
    "lambda_pred = [0, 0.1, 0.5, 1, 5]\n",
    "\n",
    "label_names = ['still', 'move', 'wheel-turn', 'groom']\n",
    "device = 'cuda'\n",
    "batch_size = 2000\n",
    "trial_splits = '9;1;0;0'\n",
    "\n",
    "\n",
    "hparams = {\n",
    "    'rng_seed_train': 0,\n",
    "    'rng_seed_model': 0,\n",
    "    'trial_splits': trial_splits,\n",
    "    'train_frac': 1,\n",
    "    'batch_size': batch_size,\n",
    "    'learning_rate': 1e-4,\n",
    "    'n_hid_layers': 2,\n",
    "    'n_hid_units': 32,\n",
    "    'n_lags': 4,\n",
    "    'l2_reg': 0,\n",
    "    'lambda_strong': lambda_strong,\n",
    "    'bidirectional': True,\n",
    "    'device': device,\n",
    "    'dropout': 0.1,\n",
    "    'batch_pad': 24,\n",
    "}\n",
    "    \n",
    "metrics_df = []\n",
    "\n",
    "for expt_id_test in expt_ids_test:\n",
    "    \n",
    "    # DLC markers\n",
    "    markers_file = os.path.join(base_dir, 'markers', expt_id_test + '_labeled.npy')\n",
    "    # hand labels\n",
    "    hand_labels_file = os.path.join(\n",
    "        base_dir, 'labels_deepethogram', 'DATA', expt_id_test, expt_id_test + '_labels.csv')\n",
    "    # heuristic labels\n",
    "    labels_file = os.path.join(\n",
    "        base_dir, 'labels-heuristic', expt_id_test + '_labels.pkl')\n",
    "\n",
    "    # define data generator signals\n",
    "    signals = ['markers']\n",
    "    transforms = [ZScore()]\n",
    "    paths = [markers_file]\n",
    "\n",
    "    # build data generator\n",
    "    data_gen_test = DataGenerator(\n",
    "        [expt_id_test], [signals], [transforms], [paths], device=device, batch_size=batch_size, \n",
    "        trial_splits=trial_splits, batch_pad=hparams['batch_pad'])\n",
    "    print('----------------------------')\n",
    "    print(data_gen_test)\n",
    "    print('----------------------------')\n",
    "    print('\\n')\n",
    "\n",
    "    # load hand labels\n",
    "    labels = genfromtxt(hand_labels_file, delimiter=',', dtype=np.int, encoding=None)\n",
    "    labels = labels[1:, 1:]  # get rid of headers, etc.\n",
    "    states = np.argmax(labels, axis=1)\n",
    "    cutoff = int(np.floor(states.shape[0] / batch_size)) * batch_size\n",
    "    states = states[:cutoff]\n",
    "\n",
    "    # load heuristic labels\n",
    "    with open(labels_file, 'rb') as f:\n",
    "        states_heuristic = pickle.load(f)['states']\n",
    "        states_heuristic = states_heuristic[:cutoff]\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # collect results\n",
    "    # -----------------------------------------        \n",
    "    for lw in lambda_weak:\n",
    "        for lp in lambda_pred:\n",
    "\n",
    "            hparams['lambda_weak'] = lw\n",
    "            hparams['lambda_pred'] = lp\n",
    "            \n",
    "            # f1, precision, accuracy\n",
    "            scores = {m: None for m in model_types}\n",
    "            # ratio of time spent in each state\n",
    "            ratios = {m: None for m in model_types}\n",
    "            # median bout length for each state\n",
    "            bout_lens = {m: None for m in model_types}\n",
    "            \n",
    "            for model_type in model_types:\n",
    "                \n",
    "                activation = 'lrelu' if model_type == 'dtcn' else 'tanh'\n",
    "        \n",
    "                # print('collecting %s results' % model_type)\n",
    "                hparams['activation'] = activation\n",
    "                hparams['model_type'] = model_type\n",
    "                hparams['tt_expt_dir'] = os.path.join(expt_dir, model_type, tt_expt_dir)\n",
    "\n",
    "                # load states/model\n",
    "                version_int = find_experiment(hparams)\n",
    "                if version_int is None:\n",
    "                    continue\n",
    "                version_str = str('version_%i' % version_int)\n",
    "                version_dir = os.path.join(hparams['tt_expt_dir'], version_str)\n",
    "                \n",
    "                # check to see if states exist\n",
    "                states_file = os.path.join(version_dir, '%s_states.npy' % expt_id_test)\n",
    "                if os.path.exists(states_file) and not overwrite_states:\n",
    "                    predictions = np.load(states_file)\n",
    "                else:\n",
    "                    # load model\n",
    "                    model_file = os.path.join(version_dir, 'best_val_model.pt')\n",
    "                    arch_file = os.path.join(version_dir, 'hparams.pkl')\n",
    "                    # print('Loading model defined in %s' % arch_file)\n",
    "                    with open(arch_file, 'rb') as f:\n",
    "                        hparams_new = pickle.load(f)\n",
    "                    hparams_new['device'] = hparams.get('device', 'cpu')\n",
    "                    model = Segmenter(hparams_new)\n",
    "                    model.load_state_dict(torch.load(\n",
    "                        model_file, map_location=lambda storage, loc: storage))\n",
    "                    model.to(hparams_new['device'])\n",
    "                    model.eval()\n",
    "\n",
    "                    # compute predictions\n",
    "                    predictions = model.predict_labels(data_gen_test)['labels']\n",
    "                    predictions = np.argmax(np.vstack(predictions[0]), axis=1)\n",
    "                    \n",
    "                    # save predictions\n",
    "                    if save_states:\n",
    "                        print('saving states to %s' % states_file)\n",
    "                        np.save(states_file, predictions)\n",
    "\n",
    "                # compute precision and recall for each behavior type (model)\n",
    "                scores[model_type] = get_precision_recall(\n",
    "                    states, predictions, background=0, n_classes=4)\n",
    "                \n",
    "                if compute_state_stats:\n",
    "                    ratios[model_type] = np.bincount(predictions) / len(predictions)\n",
    "                    bouts = run_lengths(predictions)\n",
    "                    bout_lens[model_type] = [np.median(a) for _, a in bouts.items()]\n",
    "                else:\n",
    "                    ratios[model_type] = [None] * (len(label_names) + 1)\n",
    "                    bout_lens[model_type] = [None] * (len(label_names) + 1)\n",
    "\n",
    "            # TODO: temporary\n",
    "            if version_int is None:\n",
    "                continue\n",
    "                \n",
    "            # compute precision and recall for each behavior type (heuristic)\n",
    "            scores['heuristic'] = get_precision_recall(\n",
    "                states, states_heuristic, background=0, n_classes=4)\n",
    "\n",
    "            # store\n",
    "            for l, label_name in enumerate(label_names):\n",
    "                df_dict = {\n",
    "                    'expt_id': expt_id_test,\n",
    "                    'label': label_name,\n",
    "                    'lambda_weak': lw,\n",
    "                    'lambda_pred': lp,\n",
    "                    'precision_heur': scores['heuristic']['precision'][l],\n",
    "                    'recall_heur': scores['heuristic']['recall'][l],\n",
    "                    'f1_heur': scores['heuristic']['f1'][l],\n",
    "                }\n",
    "                for model_type in model_types:\n",
    "                    model_name = 'mlp' if model_type == 'temporal-mlp' else model_type\n",
    "                    df_dict['precision_%s' % model_name] = scores[model_type]['precision'][l]\n",
    "                    df_dict['recall_%s' % model_name] = scores[model_type]['recall'][l]\n",
    "                    df_dict['f1_%s' % model_name] = scores[model_type]['f1'][l]\n",
    "                    df_dict['ratio_%s' % model_name] = ratios[model_type][l + 1]\n",
    "                    df_dict['bout_len_%s' % model_name] = bout_lens[model_type][l + 1]\n",
    "                metrics_df.append(pd.DataFrame(df_dict, index=[0]))\n",
    "\n",
    "metrics_df = pd.concat(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot results: heat plots for single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "save_figs = False\n",
    "metric = 'f1_dtcn'\n",
    "\n",
    "n_rows = 2\n",
    "n_cols = 2\n",
    "\n",
    "# 1: by dataset/label\n",
    "# 2: by label, avg over datasets\n",
    "# 3: avg over dataset/label\n",
    "plot_types = [2, 3]\n",
    "\n",
    "if save_figs:\n",
    "    fig_save_path = '/home/mattw/Dropbox/research-text/papers/2021-daart/figs'\n",
    "    fig_save_file = os.path.join(fig_save_path, '%s.pdf' % metric)\n",
    "else:\n",
    "    fig_save_file = None\n",
    "\n",
    "for plot_type in plot_types:\n",
    "    annot = True #True if plot_type == 3 else False\n",
    "    plot_heatmaps(\n",
    "        df=metrics_df, metric=metric, expt_ids=expt_ids_test, \n",
    "        title=metric.split('_')[-1].upper(), kind=plot_type, vmin=0.7, vmax=1,\n",
    "        annot=annot, cmaps=['Oranges_r', 'Greens_r', 'Reds_r', 'Purples_r'],\n",
    "        save_file=fig_save_file) #cmap='Oranges_r')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daart",
   "language": "python",
   "name": "daart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
